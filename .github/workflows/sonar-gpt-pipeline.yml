name: Fullstack Security Analysis with SonarQube + GPT-4 (Chunked)

on:
  workflow_dispatch:
  push:
    branches: [main]

jobs:
  analyze:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Setup .NET SDK
        uses: actions/setup-dotnet@v3
        with:
          dotnet-version: '8.x'

      - name: Install SonarScanner and jq
        run: |
          dotnet tool install --global dotnet-sonarscanner
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Begin SonarQube Scan
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
        run: |
          dotnet sonarscanner begin \
            /k:"eshop" \
            /d:sonar.login="${SONAR_TOKEN}" \
            /d:sonar.host.url="${SONAR_HOST_URL}" \
            /d:sonar.verbose=true \
            /d:sonar.exclusions="**/obj/**,**/bin/**,**/*.json,**/BlazorAdmin/**,infra/core/database/sqlserver/**,infra/core/security/keyvault.bicep,infra/core/host/appservice.bicep"

      - name: Clean, Restore and Build
        run: |
          dotnet clean eShopOnWeb.sln
          dotnet restore eShopOnWeb.sln
          dotnet build eShopOnWeb.sln --no-incremental

      - name: End SonarQube Scan
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        run: |
          dotnet sonarscanner end /d:sonar.login="${SONAR_TOKEN}"

      - name: Wait for SonarQube to finalize
        run: sleep 60

      - name: Create report directory structure
        run: |
          TIMESTAMP=$(date +%Y%m%d)
          mkdir -p report/$TIMESTAMP/gpt-summaries
          mkdir -p report/$TIMESTAMP/chunks
          mkdir -p report/$TIMESTAMP/results

      - name: Download SonarQube Issues
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
        run: |
          PROJECT_KEY="eshop"
          AUTH_HEADER="Authorization: Basic $(echo -n "${SONAR_TOKEN}:" | base64)"
          HTTP_STATUS=$(curl -s -o sonar-report.json -w "%{http_code}" -H "$AUTH_HEADER" "${SONAR_HOST_URL}/api/issues/search?componentKeys=${PROJECT_KEY}&ps=500")
          if [ "$HTTP_STATUS" -ne 200 ]; then
            echo "ERROR: SonarQube API returned HTTP status $HTTP_STATUS"
            cat sonar-report.json
            exit 1
          fi

      - name: Verify SonarQube Report Content
        run: |
          if [ ! -s sonar-report.json ] || ! jq empty sonar-report.json > /dev/null 2>&1; then
            echo "WARNING: SonarQube report is empty or not valid JSON"
            echo '{"issues":[],"components":[],"total":0}' > sonar-report.json
            echo "Created empty report template"
          fi
          
          ISSUE_COUNT=$(jq '.issues | length' sonar-report.json)
          COMPONENT_COUNT=$(jq '.components | length' sonar-report.json)
          TOTAL_COUNT=$(jq '.total' sonar-report.json)
          echo "SonarQube report contains: $ISSUE_COUNT issues, $COMPONENT_COUNT components, $TOTAL_COUNT total reported issues"

      - name: Upload SonarQube Raw Report to Azure Blob
        env:
          AZURE_BLOB_SAS_URL: ${{ secrets.AZURE_BLOB_SAS_URL }}
        run: |
          jq '.' sonar-report.json > sonar-report-pretty.json
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          FILENAME="sonar-report-${TIMESTAMP}.json"
          SAS_URL="${AZURE_BLOB_SAS_URL}"
          BASE_URL=$(echo "$SAS_URL" | cut -d'?' -f1)
          SAS_TOKEN=$(echo "$SAS_URL" | cut -d'?' -f2-)
          FULL_URL="${BASE_URL}/${FILENAME}?${SAS_TOKEN}"
          curl -s -S -f -X PUT "$FULL_URL" -H "x-ms-blob-type: BlockBlob" --data-binary @sonar-report-pretty.json
          cp sonar-report-pretty.json "report/$(date +%Y%m%d)/${FILENAME}"
          echo "SonarQube report saved as $FILENAME"

      - name: Create report directory structure and export variables
        run: |
          # Create timestamp for this run
          FOLDER_DATE=$(date +%Y%m%d)
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          
          # Define directories for chunks and results
          CHUNKS_DIR="report/${FOLDER_DATE}/chunks"
          RESULTS_DIR="report/${FOLDER_DATE}/results"
          
          # Create directory structure
          mkdir -p "$CHUNKS_DIR"
          mkdir -p "$RESULTS_DIR"
          
          # Export variables for use in subsequent steps
          echo "FOLDER_DATE=${FOLDER_DATE}" >> $GITHUB_ENV
          echo "TIMESTAMP=${TIMESTAMP}" >> $GITHUB_ENV
          echo "CHUNKS_DIR=${CHUNKS_DIR}" >> $GITHUB_ENV
          echo "RESULTS_DIR=${RESULTS_DIR}" >> $GITHUB_ENV
          
          # Create base prompt file
          cp .github/workflows/base-prompt.txt base-prompt.txt

      - name: Split SonarQube report into chunks for GPT processing
        run: |
          # Extract issues and components separately
          jq '.issues' sonar-report.json > all-issues.json
          jq '.components' sonar-report.json > all-components.json
          
          # Count issues
          TOTAL_ISSUES=$(jq '. | length' all-issues.json)
          echo "Total issues: $TOTAL_ISSUES"
          echo "TOTAL_ISSUES=${TOTAL_ISSUES}" >> $GITHUB_ENV
          
          # Define chunk size (number of issues per chunk)
          # Use single-issue chunks by default to prevent timeout errors
          CHUNK_SIZE=1
          echo "CHUNK_SIZE=${CHUNK_SIZE}" >> $GITHUB_ENV
          
          # Calculate number of chunks needed
          CHUNKS=$(( (TOTAL_ISSUES + CHUNK_SIZE - 1) / CHUNK_SIZE ))
          echo "Splitting into ${CHUNKS} chunks of ~${CHUNK_SIZE} issues each"
          echo "CHUNKS=${CHUNKS}" >> $GITHUB_ENV
          
          # Create ultra-minimal prompt to prevent 504 Gateway Timeout errors
          echo "Analyze SonarQube issue(s) (chunk {CHUNK_NUM}/{TOTAL_CHUNKS}):" > base-prompt.txt
          echo "- List severity" >> base-prompt.txt
          echo "- File location" >> base-prompt.txt 
          echo "- Fix recommendation" >> base-prompt.txt
          echo "Extremely brief! 1-2 sentences per issue. Use bullet points only." >> base-prompt.txt
          
          # Create chunks by slicing the issues array
          for (( i=0; i<$CHUNKS; i++ ))
          do
              START=$(( i * CHUNK_SIZE ))
              # Create a JSON containing a slice of the issues array
              jq -c ".[$START:$(( START + CHUNK_SIZE ))]" all-issues.json > "${CHUNKS_DIR}/issues-chunk-${i}.json"
              
              # Prepare chunk-specific prompt by replacing placeholders
              CHUNK_NUM=$((i + 1))
              sed "s/{CHUNK_NUM}/$CHUNK_NUM/g; s/{TOTAL_CHUNKS}/$CHUNKS/g" base-prompt.txt > "${CHUNKS_DIR}/prompt-${i}.txt"
              
              # Create ultra-minimal issue payload
              echo "Optimizing issue data to prevent timeouts"
              jq '[.[] | {key, rule, severity, component, message}]' "${CHUNKS_DIR}/issues-chunk-${i}.json" > "${CHUNKS_DIR}/issues-optimized-${i}.json"
              mv "${CHUNKS_DIR}/issues-optimized-${i}.json" "${CHUNKS_DIR}/issues-chunk-${i}.json"
              
              # Create streamlined components (only include keys and paths)
              jq '[.[] | {key, path}]' all-components.json > "${CHUNKS_DIR}/components-optimized.json"
              
              # Create the GPT payload for this chunk with ultra-minimal format
              jq -n \
                --slurpfile issues "${CHUNKS_DIR}/issues-chunk-${i}.json" \
                --rawfile prompt "${CHUNKS_DIR}/prompt-${i}.txt" \
                '{
                  "prompt": $prompt,
                  "issues": ($issues[0] | map({
                    rule: .rule,
                    severity: .severity,
                    component: (if .component then (.component | split(":") | last) else "Unknown file" end),
                    message: (if .message then (if (.message | length) > 100 then (.message[0:100] + "...") else .message end) else "No message available" end)
                  })),
                  "metadata": {
                    "chunk": '"$CHUNK_NUM"',
                    "total": '"$CHUNKS"'
                  }
                }' > "${CHUNKS_DIR}/gpt-payload-${i}.json"
              
              # Validate the generated JSON
              if ! jq empty "${CHUNKS_DIR}/gpt-payload-${i}.json" > /dev/null 2>&1; then
                  echo "ERROR: Generated invalid JSON payload for chunk $i"
                  exit 1
              fi
              
              # Log chunk creation
              CHUNK_SIZE_BYTES=$(wc -c < "${CHUNKS_DIR}/gpt-payload-${i}.json")
              CHUNK_ISSUES=$(jq 'length' "${CHUNKS_DIR}/issues-chunk-${i}.json")
              echo "Created chunk $i: $CHUNK_SIZE_BYTES bytes, $CHUNK_ISSUES issues"
          done
          
          # Validate all chunks were created properly
          CREATED_CHUNKS=$(ls -l "${CHUNKS_DIR}/gpt-payload-"* | wc -l)
          if [ "$CREATED_CHUNKS" -ne "$CHUNKS" ]; then
              echo "ERROR: Expected to create $CHUNKS chunks but found $CREATED_CHUNKS"
              ls -l "${CHUNKS_DIR}/gpt-payload-"*
              exit 1
          fi
          
          echo "Successfully split SonarQube report into $CHUNKS chunks"
          
      - name: Process chunks with GPT-4 API
        env:
          GPT_FUNCTION_ENDPOINT: ${{ secrets.AZURE_FUNCTION_URL }}
        run: |
          # Define retry parameters
          MAX_RETRIES=3
          BASE_TIMEOUT=120
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          
          # Function to process a single chunk
          process_chunk() {
              CHUNK_INDEX=$1
              CHUNK_FILE="${CHUNKS_DIR}/gpt-payload-${CHUNK_INDEX}.json"
              OUTPUT_FILE="${RESULTS_DIR}/gpt-result-${CHUNK_INDEX}.txt"
              SUMMARY_FILE="${RESULTS_DIR}/gpt-summary-${CHUNK_INDEX}.md"
              ERROR_LOG_FILE="${RESULTS_DIR}/curl-error-${CHUNK_INDEX}.txt"
              RESPONSE_LOG_FILE="${RESULTS_DIR}/response-body-${CHUNK_INDEX}.txt"
              
              echo "Processing chunk ${CHUNK_INDEX} (Attempt 1/${MAX_RETRIES})"
              
              # Set dynamic timeout based on chunk size
              CHUNK_SIZE_BYTES=$(wc -c < "$CHUNK_FILE")
              # More progressive timeout scaling based on payload size
              if [ "$CHUNK_SIZE_BYTES" -gt 2000000 ]; then
                  TIMEOUT=$((BASE_TIMEOUT * 5))
              elif [ "$CHUNK_SIZE_BYTES" -gt 1000000 ]; then
                  TIMEOUT=$((BASE_TIMEOUT * 3))
              elif [ "$CHUNK_SIZE_BYTES" -gt 500000 ]; then
                  TIMEOUT=$((BASE_TIMEOUT * 2))
              else
                  TIMEOUT=$BASE_TIMEOUT
              fi
              echo "Chunk size: $CHUNK_SIZE_BYTES bytes, timeout: $TIMEOUT seconds"
              
              # Verify chunk file is valid JSON
              if ! jq empty "$CHUNK_FILE" > /dev/null 2>&1; then
                echo "ERROR: Chunk payload is not valid JSON"
                jq -r . "$CHUNK_FILE" > "${RESULTS_DIR}/invalid-json-chunk-${CHUNK_INDEX}.txt" 2>&1
                return 1
              fi
              
              # Validate the structure of the chunk file to ensure it has all required fields
              if ! jq -e 'has("prompt") and has("issues")' "$CHUNK_FILE" > /dev/null 2>&1; then
                echo "ERROR: Chunk payload is missing required fields (prompt, issues)"
                return 1
              fi
              
              # Process with retries and backoff
              for try in $(seq 1 $MAX_RETRIES); do
                  if [ $try -gt 1 ]; then
                      SLEEP_TIME=$(( (try - 1) * 15 ))
                      echo "Retrying chunk ${CHUNK_INDEX} after ${SLEEP_TIME}s delay (Attempt ${try}/${MAX_RETRIES})"
                      sleep $SLEEP_TIME
                  fi
                  
                  # Call the GPT-4 API with verbose flag for more debugging info
                  HTTP_CODE=$(curl -v -s -o "$RESPONSE_LOG_FILE" -w "%{http_code}" \
                      -X POST "${GPT_FUNCTION_ENDPOINT}" \
                      -H "Content-Type: application/json" \
                      --data-binary @"$CHUNK_FILE" \
                      --max-time $TIMEOUT 2> "$ERROR_LOG_FILE")
                  
                  # Check if call was successful
                  if [ "$HTTP_CODE" -eq 200 ]; then
                      # Check if response contains actual content (not just whitespace)
                      if [ -s "$RESPONSE_LOG_FILE" ] && grep -q '[a-zA-Z0-9]' "$RESPONSE_LOG_FILE"; then
                          # Copy the response to the final summary file
                          cp "$RESPONSE_LOG_FILE" "$SUMMARY_FILE"
                          echo "Successfully processed chunk ${CHUNK_INDEX} (Attempt ${try}/${MAX_RETRIES})"
                          return 0
                      else
                          echo "Warning: Empty or whitespace-only response from API for chunk ${CHUNK_INDEX}"
                          # Continue to the next retry
                      fi
                  else
                      echo "Failed to process chunk ${CHUNK_INDEX}: HTTP code ${HTTP_CODE} (Attempt ${try}/${MAX_RETRIES})"
                      # Debug error info
                      if [ -s "$ERROR_LOG_FILE" ]; then
                          echo "Error log:"
                          cat "$ERROR_LOG_FILE"
                      fi
                      # If we got a response body, log it too
                      if [ -s "$RESPONSE_LOG_FILE" ]; then
                          echo "Response body:"
                          cat "$RESPONSE_LOG_FILE"
                      fi
                  fi
              done
              
              # All retries failed
              echo "# GPT-4 Analysis Failed: Chunk ${CHUNK_INDEX}" > "$SUMMARY_FILE"
              echo "" >> "$SUMMARY_FILE"
              echo "Failed to process chunk ${CHUNK_INDEX} after ${MAX_RETRIES} attempts." >> "$SUMMARY_FILE"
              echo "" >> "$SUMMARY_FILE"
              echo "### Error Details" >> "$SUMMARY_FILE"
              echo "" >> "$SUMMARY_FILE"
              echo '```' >> "$SUMMARY_FILE"
              cat "$ERROR_LOG_FILE" >> "$SUMMARY_FILE"
              echo '```' >> "$SUMMARY_FILE"
              return 1
          }
          
          # Process all chunks
          FAILED_CHUNKS=0
          SUCCESSFUL_CHUNKS=0
          
          echo "Processing ${CHUNKS} chunks..."
          
          # Process chunks in sequence to prevent overwhelming the API
          for (( i=0; i<$CHUNKS; i++ ))
          do
              if process_chunk $i; then
                  SUCCESSFUL_CHUNKS=$((SUCCESSFUL_CHUNKS + 1))
                  echo "Chunk $i completed successfully ($SUCCESSFUL_CHUNKS/$CHUNKS completed)"
              else
                  FAILED_CHUNKS=$((FAILED_CHUNKS + 1))
                  echo "Chunk $i failed ($FAILED_CHUNKS/$CHUNKS failed)"
              fi
          done
          
          echo "Chunk processing complete: $SUCCESSFUL_CHUNKS successful, $FAILED_CHUNKS failed"
          echo "SUCCESSFUL_CHUNKS=${SUCCESSFUL_CHUNKS}" >> $GITHUB_ENV
          echo "FAILED_CHUNKS=${FAILED_CHUNKS}" >> $GITHUB_ENV

      - name: Aggregate chunk results into final report
        env:
          GPT_FUNCTION_ENDPOINT: ${{ secrets.AZURE_FUNCTION_URL }}
        run: |
          echo "Aggregating chunk results into final report"
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          COMBINED_SUMMARY="${RESULTS_DIR}/gpt-final-summary-${TIMESTAMP}.md"
          
          # Check if we have enough successful chunks
          TOTAL_SUMMARY_FILES=$(find "${RESULTS_DIR}" -name "gpt-summary-*.md" | wc -l)
          if [ $TOTAL_SUMMARY_FILES -lt $(($CHUNKS / 2)) ]; then
            echo "ERROR: Too many failed chunks, cannot generate meaningful combined report"
            echo "# GPT-4 Security Analysis - Incomplete" > "$COMBINED_SUMMARY"
            echo "" >> "$COMBINED_SUMMARY"
            echo "Not enough chunks were successfully processed to generate a meaningful report." >> "$COMBINED_SUMMARY"
            echo "" >> "$COMBINED_SUMMARY"
            echo "Successfully processed: ${SUCCESSFUL_CHUNKS}/${CHUNKS} chunks" >> "$COMBINED_SUMMARY"
          else
            # For small number of chunks, combine directly
            if [ $CHUNKS -le 5 ]; then
              echo "Combining a small number of chunks directly"
              echo "# Combined Security Analysis Report" > "$COMBINED_SUMMARY"
              echo "" >> "$COMBINED_SUMMARY"
              echo "This report combines analysis from ${SUCCESSFUL_CHUNKS} successful chunks." >> "$COMBINED_SUMMARY"
              echo "" >> "$COMBINED_SUMMARY"
              
              # Add each chunk to the report
              for (( i=0; i<$CHUNKS; i++ ))
              do
                CHUNK_SUMMARY="${RESULTS_DIR}/gpt-summary-${i}.md"
                if [ -f "$CHUNK_SUMMARY" ]; then
                  echo "## Chunk $((i + 1)) Analysis" >> "$COMBINED_SUMMARY"
                  echo "" >> "$COMBINED_SUMMARY"
                  cat "$CHUNK_SUMMARY" >> "$COMBINED_SUMMARY"
                  echo "" >> "$COMBINED_SUMMARY"
                  echo "---" >> "$COMBINED_SUMMARY"
                  echo "" >> "$COMBINED_SUMMARY"
                else
                  echo "## Chunk $((i + 1)) Analysis" >> "$COMBINED_SUMMARY"
                  echo "" >> "$COMBINED_SUMMARY"
                  echo "Analysis for this chunk is not available." >> "$COMBINED_SUMMARY"
                  echo "" >> "$COMBINED_SUMMARY"
                  echo "---" >> "$COMBINED_SUMMARY"
                  echo "" >> "$COMBINED_SUMMARY"
                fi
              done
            else
              # For larger numbers of chunks, use GPT-4 for synthesis
              echo "Using GPT to synthesize results from ${SUCCESSFUL_CHUNKS} successful chunks"
              
              # Create synthesis JSON file - write to file directly instead of using heredoc
              echo "{" > synthesis-prompt.json
              echo '  "prompt": "As a security expert, synthesize the following individual security analysis chunks into one cohesive report. Focus on:\n1. Top critical and major issues across all chunks\n2. Common patterns of security issues\n3. Key recommendations that address multiple issues\n\nProvide a concise executive summary followed by specific findings and actionable recommendations.",' >> synthesis-prompt.json
              echo '  "chunkResults": [' >> synthesis-prompt.json
              
              # Add each successful chunk to synthesis input
              FIRST=true
              for (( i=0; i<$CHUNKS; i++ ))
              do
                CHUNK_SUMMARY="${RESULTS_DIR}/gpt-summary-${i}.md"
                if [ -f "$CHUNK_SUMMARY" ] && [ -s "$CHUNK_SUMMARY" ]; then
                  if ! grep -q "Failed to process chunk" "$CHUNK_SUMMARY"; then
                    if [ "$FIRST" = true ]; then
                      FIRST=false
                    else
                      echo "," >> synthesis-prompt.json
                    fi
                    jq -Rs '    { "chunk": . }' "$CHUNK_SUMMARY" >> synthesis-prompt.json
                  fi
                fi
              done
              
              # Close the JSON structure
              echo "  ]" >> synthesis-prompt.json
              echo "}" >> synthesis-prompt.json
              
              # Validate JSON and process
              if ! jq empty synthesis-prompt.json > /dev/null 2>&1; then
                echo "ERROR: Generated invalid JSON synthesis prompt"
                echo "# GPT-4 Security Analysis - Synthesis Failed" > "$COMBINED_SUMMARY"
                echo "" >> "$COMBINED_SUMMARY"
                echo "Failed to generate a valid synthesis prompt." >> "$COMBINED_SUMMARY"
                echo "" >> "$COMBINED_SUMMARY"
                echo "Individual chunk analyses are available." >> "$COMBINED_SUMMARY"
              else
                # Try to synthesize with GPT-4
                echo "Calling GPT-4 for final synthesis..."
                MAX_RETRIES=3
                SYNTHESIS_SUCCESS=false
                
                for try in $(seq 1 $MAX_RETRIES); do
                  if [ $try -gt 1 ]; then
                    SLEEP_TIME=$(( (try - 1) * 30 ))
                    echo "Retrying synthesis after ${SLEEP_TIME}s delay (Attempt ${try}/${MAX_RETRIES})"
                    sleep $SLEEP_TIME
                  fi
                  
                  # Call GPT-4 for synthesis
                  HTTP_CODE=$(curl -s -o synthesis-result.md -w "%{http_code}" \
                    -X POST "${GPT_FUNCTION_ENDPOINT}" \
                    -H "Content-Type: application/json" \
                    --data-binary @synthesis-prompt.json \
                    --max-time 300)
                  
                  if [ "$HTTP_CODE" -eq 200 ] && [ -s synthesis-result.md ]; then
                    echo "Synthesis successful!"
                    SYNTHESIS_SUCCESS=true
                    break
                  else
                    echo "Synthesis attempt ${try} failed: HTTP code ${HTTP_CODE}"
                  fi
                done
                
                # Create final report based on synthesis result
                if [ "$SYNTHESIS_SUCCESS" = true ]; then
                  echo "# GPT-4 Security Analysis - Synthesized Report" > "$COMBINED_SUMMARY"
                  echo "" >> "$COMBINED_SUMMARY"
                  echo "This report synthesizes findings from ${SUCCESSFUL_CHUNKS}/${CHUNKS} successful chunks." >> "$COMBINED_SUMMARY"
                  echo "" >> "$COMBINED_SUMMARY"
                  cat synthesis-result.md >> "$COMBINED_SUMMARY"
                else
                  # Manual aggregation fallback
                  echo "# GPT-4 Security Analysis - Manual Aggregation" > "$COMBINED_SUMMARY"
                  echo "" >> "$COMBINED_SUMMARY"
                  echo "GPT synthesis failed. This report combines individual chunk analyses without synthesis." >> "$COMBINED_SUMMARY"
                  echo "" >> "$COMBINED_SUMMARY"
                  
                  for (( i=0; i<$CHUNKS; i++ ))
                  do
                    CHUNK_SUMMARY="${RESULTS_DIR}/gpt-summary-${i}.md"
                    if [ -f "$CHUNK_SUMMARY" ]; then
                      echo "## Chunk $((i + 1)) Analysis" >> "$COMBINED_SUMMARY"
                      echo "" >> "$COMBINED_SUMMARY"
                      cat "$CHUNK_SUMMARY" >> "$COMBINED_SUMMARY"
                      echo "" >> "$COMBINED_SUMMARY"
                      echo "---" >> "$COMBINED_SUMMARY"
                      echo "" >> "$COMBINED_SUMMARY"
                    fi
                  done
                fi
              fi
            fi
          fi
          
          echo "Final report generated at: $COMBINED_SUMMARY"
          echo "FINAL_REPORT=${COMBINED_SUMMARY}" >> $GITHUB_ENV

      - name: Upload Final GPT-4 Report to Azure Blob
        env:
          AZURE_BLOB_SAS_URL: ${{ secrets.AZURE_BLOB_SAS_URL }}
        run: |
          FILENAME=$(basename "$FINAL_REPORT")
          SAS_URL="${AZURE_BLOB_SAS_URL}"
          BASE_URL=$(echo "$SAS_URL" | cut -d'?' -f1)
          SAS_TOKEN=$(echo "$SAS_URL" | cut -d'?' -f2-)
          FULL_URL="${BASE_URL}/${FILENAME}?${SAS_TOKEN}"
          
          echo "Uploading final GPT-4 security analysis report..."
          curl -s -S -f -X PUT "$FULL_URL" -H "x-ms-blob-type: BlockBlob" --data-binary @"$FINAL_REPORT"
          echo "Uploaded final report to Azure Blob: $FILENAME"
