# Security Analysis Pipeline with SonarQube + GPT-4 + Azure Blob Upload
# Enhanced with report chunking to handle large SonarQube reports exceeding GPT-4 token limits
name: Fullstack Security Analysis with SonarQube + GPT-4 (Chunked)

on:
  workflow_dispatch:
  push:
    branches: [main]

jobs:
  analyze:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Setup .NET SDK
        uses: actions/setup-dotnet@v3
        with:
          dotnet-version: '8.x'

      - name: Install SonarScanner and jq
        run: |
          dotnet tool install --global dotnet-sonarscanner
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Begin SonarQube Scan
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
        run: |
          dotnet sonarscanner begin \
            /k:"eshop" \
            /d:sonar.login="${SONAR_TOKEN}" \
            /d:sonar.host.url="${SONAR_HOST_URL}" \
            /d:sonar.verbose=true \
            /d:sonar.exclusions="**/obj/**,**/bin/**,**/*.json,**/BlazorAdmin/**,infra/core/database/sqlserver/**,infra/core/security/keyvault.bicep,infra/core/host/appservice.bicep"

      - name: Clean, Restore and Build
        run: |
          # Explicitly specify solution file to avoid "more than one project" error
          dotnet clean eShopOnWeb.sln
          dotnet restore eShopOnWeb.sln
          dotnet build eShopOnWeb.sln --no-incremental

      - name: End SonarQube Scan
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        run: |
          dotnet sonarscanner end /d:sonar.login="${SONAR_TOKEN}"

      - name: Wait for SonarQube to finalize
        run: sleep 60

      - name: Create report directory structure
        run: |
          TIMESTAMP=$(date +%Y%m%d)
          mkdir -p report/$TIMESTAMP/gpt-summaries
          mkdir -p report/$TIMESTAMP/chunks
          mkdir -p report/$TIMESTAMP/results

      - name: Download SonarQube Issues
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
        run: |
          PROJECT_KEY="eshop"
          AUTH_HEADER="Authorization: Basic $(echo -n "${SONAR_TOKEN}:" | base64)"
          HTTP_STATUS=$(curl -s -o sonar-report.json -w "%{http_code}" -H "$AUTH_HEADER" "${SONAR_HOST_URL}/api/issues/search?componentKeys=${PROJECT_KEY}&ps=500")
          if [ "$HTTP_STATUS" -ne 200 ]; then
            echo "ERROR: SonarQube API returned HTTP status $HTTP_STATUS"
            cat sonar-report.json
            exit 1
          fi

      - name: Verify SonarQube Report Content
        run: |
          if [ ! -s sonar-report.json ] || ! jq empty sonar-report.json > /dev/null 2>&1; then
            echo "WARNING: SonarQube report is empty or not valid JSON"
            echo '{"issues":[],"components":[],"total":0}' > sonar-report.json
            echo "Created empty report template"
          fi
          
          # Log report information for diagnostics
          ISSUE_COUNT=$(jq '.issues | length' sonar-report.json)
          COMPONENT_COUNT=$(jq '.components | length' sonar-report.json)
          TOTAL_COUNT=$(jq '.total' sonar-report.json)
          echo "SonarQube report contains: $ISSUE_COUNT issues, $COMPONENT_COUNT components, $TOTAL_COUNT total reported issues"
          
          # Check for empty/incomplete report
          if [ "$ISSUE_COUNT" -eq 0 ]; then
            echo "WARNING: No issues found in SonarQube report. This might cause problems with the GPT analysis."
          fi

      - name: Upload SonarQube Raw Report to Azure Blob
        env:
          AZURE_BLOB_SAS_URL: ${{ secrets.AZURE_BLOB_SAS_URL }}
        run: |
          jq '.' sonar-report.json > sonar-report-pretty.json
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          FILENAME="sonar-report-${TIMESTAMP}.json"
          SAS_URL="${AZURE_BLOB_SAS_URL}"
          BASE_URL=$(echo "$SAS_URL" | cut -d'?' -f1)
          SAS_TOKEN=$(echo "$SAS_URL" | cut -d'?' -f2-)
          FULL_URL="${BASE_URL}/${FILENAME}?${SAS_TOKEN}"
          curl -s -S -f -X PUT "$FULL_URL" -H "x-ms-blob-type: BlockBlob" --data-binary @sonar-report-pretty.json
          
          FOLDER_DATE=$(date +%Y%m%d)
          cp sonar-report-pretty.json "report/${FOLDER_DATE}/${FILENAME}"
          echo "SonarQube report saved to report/${FOLDER_DATE}/${FILENAME}"

      - name: Create Base GPT Prompt
        run: |
          # Create prompt file with detailed analysis instructions
          cat > base-prompt.txt << 'EOT'
          You are a secure code reviewer with deep knowledge of software vulnerabilities. Given the following static analysis results from SonarQube, perform the following tasks:

          1. Identify the most severe issues and explain why they are critical.
          2. Group issues by file/module and summarize their overall security and code quality state.
          3. Recommend specific refactoring or remediation actions for the top issues using secure coding best practices.

          This is chunk {CHUNK_NUM} of {TOTAL_CHUNKS} from the full report. Focus on analyzing just the issues in this chunk.
          EOT
          
          # Create final prompt template for unified analysis
          cat > final-prompt.txt << 'EOT'
          You are a secure code reviewer with deep knowledge of software vulnerabilities. Below are summaries from analyzing chunks of a SonarQube report.
          
          Your task is to create a unified, comprehensive security analysis report by combining these individual analyses.
          
          Please synthesize this information into a cohesive report that:
          1. Identifies the top 10 most severe issues across all chunks and explains why they are critical.
          2. Groups issues by file/module and summarizes their overall security and code quality state.
          3. Recommends specific refactoring or remediation actions for the top issues using secure coding best practices.
          4. Presents your output in well-structured markdown, including:
             - A prioritized issue summary table
             - A per-module security assessment
             - Actionable recommendations
          
          Do not repeat individual chunk analyses verbatim. Instead, synthesize them into a cohesive whole.

          {CHUNK_SUMMARIES}
          EOT

      - name: Split SonarQube report into chunks for GPT processing
        run: |
          # Export environment variables for use in all steps
          FOLDER_DATE=$(date +%Y%m%d)
          CHUNKS_DIR="report/${FOLDER_DATE}/chunks"
          RESULTS_DIR="report/${FOLDER_DATE}/results"
          echo "FOLDER_DATE=${FOLDER_DATE}" >> $GITHUB_ENV
          echo "CHUNKS_DIR=${CHUNKS_DIR}" >> $GITHUB_ENV
          echo "RESULTS_DIR=${RESULTS_DIR}" >> $GITHUB_ENV
          
          # Extract issues and components separately
          jq '.issues' sonar-report.json > all-issues.json
          jq '.components' sonar-report.json > all-components.json
          
          # Get total issues count to calculate chunks
          TOTAL_ISSUES=$(jq 'length' all-issues.json)
          echo "Total issues: ${TOTAL_ISSUES}"
          echo "TOTAL_ISSUES=${TOTAL_ISSUES}" >> $GITHUB_ENV
          
          # Define chunk size (number of issues per chunk)
          # Adjust this value based on token limitations
          # Start with a smaller chunk size for safer processing
          CHUNK_SIZE=15  # Reduced from 25 to 15 issues per chunk
          echo "CHUNK_SIZE=${CHUNK_SIZE}" >> $GITHUB_ENV
          
          # For very large reports, make chunks even smaller
          if [ $TOTAL_ISSUES -gt 100 ]; then
            CHUNK_SIZE=10
            echo "Large report detected ($TOTAL_ISSUES issues). Adjusted chunk size to $CHUNK_SIZE"
            echo "CHUNK_SIZE=${CHUNK_SIZE}" >> $GITHUB_ENV
          fi
          
          # Calculate number of chunks needed
          CHUNKS=$(( (TOTAL_ISSUES + CHUNK_SIZE - 1) / CHUNK_SIZE ))
          echo "Splitting into ${CHUNKS} chunks of ~${CHUNK_SIZE} issues each"
          echo "CHUNKS=${CHUNKS}" >> $GITHUB_ENV
          
          # Create chunks by slicing the issues array
          for (( i=0; i<$CHUNKS; i++ ))
          do
              START=$(( i * CHUNK_SIZE ))
              # Create a JSON containing a slice of the issues array
              jq -c ".[$START:$(( START + CHUNK_SIZE ))]" all-issues.json > "${CHUNKS_DIR}/issues-chunk-${i}.json"
              
              # Create the full chunk payload for GPT
              CHUNK_NUM=$((i + 1))
              
              # Prepare chunk-specific prompt by replacing placeholders
              sed "s/{CHUNK_NUM}/$CHUNK_NUM/g; s/{TOTAL_CHUNKS}/$CHUNKS/g" base-prompt.txt > "${CHUNKS_DIR}/prompt-${i}.txt"
              
              # Create the GPT payload for this chunk
              jq -n \
                --slurpfile issues "${CHUNKS_DIR}/issues-chunk-${i}.json" \
                --slurpfile components all-components.json \
                --rawfile prompt "${CHUNKS_DIR}/prompt-${i}.txt" \
                '{"prompt": $prompt, "issues": $issues[0], "components": $components[0]}' > "${CHUNKS_DIR}/gpt-payload-${i}.json"
              
              # Log chunk creation
              CHUNK_SIZE_BYTES=$(wc -c < "${CHUNKS_DIR}/gpt-payload-${i}.json")
              CHUNK_ISSUES=$(jq 'length' "${CHUNKS_DIR}/issues-chunk-${i}.json")
              echo "Created chunk $i: $CHUNK_SIZE_BYTES bytes, $CHUNK_ISSUES issues"
          done
          
          # Validate all chunks were created properly
          CREATED_CHUNKS=$(ls -l "${CHUNKS_DIR}/gpt-payload-"* | wc -l)
          if [ "$CREATED_CHUNKS" -ne "$CHUNKS" ]; then
              echo "ERROR: Expected to create $CHUNKS chunks but found $CREATED_CHUNKS"
              ls -l "${CHUNKS_DIR}/gpt-payload-"*
              exit 1
          fi
          
          echo "Successfully split SonarQube report into $CHUNKS chunks"

      - name: Process chunks with GPT-4 API
        env:
          GPT_FUNCTION_ENDPOINT: ${{ secrets.GPT_FUNCTION_ENDPOINT }}
          AZURE_BLOB_SAS_URL: ${{ secrets.AZURE_BLOB_SAS_URL }}
        run: |
          MAX_RETRIES=3
          BASE_TIMEOUT=120
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          
          # Function to process a single chunk
          process_chunk() {
              CHUNK_INDEX=$1
              CHUNK_FILE="${CHUNKS_DIR}/gpt-payload-${CHUNK_INDEX}.json"
              OUTPUT_FILE="${RESULTS_DIR}/gpt-result-${CHUNK_INDEX}.txt"
              SUMMARY_FILE="${RESULTS_DIR}/gpt-summary-${CHUNK_INDEX}.md"
              
              echo "Processing chunk ${CHUNK_INDEX} (Attempt 1/${MAX_RETRIES})"
              
              # Set dynamic timeout based on chunk size
              CHUNK_SIZE_BYTES=$(wc -c < "$CHUNK_FILE")
              # More progressive timeout scaling based on payload size
              if [ "$CHUNK_SIZE_BYTES" -gt 2000000 ]; then
                  TIMEOUT=$((BASE_TIMEOUT * 4))
              elif [ "$CHUNK_SIZE_BYTES" -gt 1000000 ]; then
                  TIMEOUT=$((BASE_TIMEOUT * 2))
              else
                  TIMEOUT=$BASE_TIMEOUT
              fi
              echo "Chunk size: $CHUNK_SIZE_BYTES bytes, timeout: $TIMEOUT seconds"
              
              # Verify chunk file is valid JSON
              if ! jq empty "$CHUNK_FILE" > /dev/null 2>&1; then
                echo "ERROR: Chunk payload is not valid JSON"
                return 1
              fi
              
              # Process with retries and backoff
              for try in $(seq 1 $MAX_RETRIES); do
                  if [ $try -gt 1 ]; then
                      SLEEP_TIME=$(( (try - 1) * 15 ))
                      echo "Retrying chunk ${CHUNK_INDEX} after ${SLEEP_TIME}s delay (Attempt ${try}/${MAX_RETRIES})"
                      sleep $SLEEP_TIME
                  fi
                  
                  # Debug log the chunk size and endpoint
                  echo "Submitting chunk of $(wc -c < "$CHUNK_FILE") bytes to GPT API"
                  
                  # Call the GPT-4 API with verbose flag for more debugging info
                  HTTP_CODE=$(curl -v -s -o "$OUTPUT_FILE" -w "%{http_code}" \
                      -X POST "${GPT_FUNCTION_ENDPOINT}" \
                      -H "Content-Type: application/json" \
                      --data-binary @"$CHUNK_FILE" \
                      --max-time $TIMEOUT 2> "${RESULTS_DIR}/curl-error-${CHUNK_INDEX}.txt")
                  
                  if [ "$HTTP_CODE" -eq 200 ]; then
                      echo "Chunk ${CHUNK_INDEX} processed successfully"
                      
                      # Save raw response for debugging
                      cp "$OUTPUT_FILE" "${RESULTS_DIR}/raw-response-${CHUNK_INDEX}.json"
                      
                      # Process response - enhanced parsing logic
                      if grep -q "^#" "$OUTPUT_FILE"; then
                          echo "Found markdown response"
                          # Direct markdown response
                          cat "$OUTPUT_FILE" > "$SUMMARY_FILE"
                      elif grep -q "^{" "$OUTPUT_FILE"; then
                          echo "Found JSON response"
                          # Log JSON structure for debugging
                          jq 'keys' "$OUTPUT_FILE" > "${RESULTS_DIR}/keys-${CHUNK_INDEX}.txt"
                          
                          # Extract JSON content - try multiple field patterns
                          if jq -r '.content // .result // .markdown // .text // .analysis // .output // .response // .' "$OUTPUT_FILE" > "$SUMMARY_FILE" 2>/dev/null; then
                              echo "Extracted content from JSON response for chunk ${CHUNK_INDEX}"
                          else
                              # Fall back to raw response if JSON extraction fails
                              echo "JSON extraction failed - using raw output"
                              echo "# GPT-4 Analysis: Chunk ${CHUNK_INDEX}" > "$SUMMARY_FILE"
                              cat "$OUTPUT_FILE" >> "$SUMMARY_FILE"
                          fi
                      else
                          echo "Found plain text response"
                          # Raw text response
                          echo "# GPT-4 Analysis: Chunk ${CHUNK_INDEX}" > "$SUMMARY_FILE"
                          cat "$OUTPUT_FILE" >> "$SUMMARY_FILE"
                      fi
                      
                      echo "Chunk ${CHUNK_INDEX} analysis saved to $SUMMARY_FILE"
                      return 0
                  else
                      echo "API request failed for chunk ${CHUNK_INDEX} with HTTP code ${HTTP_CODE} (attempt ${try})"
                      echo "Error details:"
                      cat "${RESULTS_DIR}/curl-error-${CHUNK_INDEX}.txt"
                      
                      if [ -s "$OUTPUT_FILE" ]; then
                          echo "Error response content (first 500 bytes):"
                          head -c 500 "$OUTPUT_FILE"
                          echo "..."
                      fi
                      
                      # Last retry failed
                      if [ $try -eq $MAX_RETRIES ]; then
                          echo "# GPT-4 Analysis Failed: Chunk ${CHUNK_INDEX}" > "$SUMMARY_FILE"
                          echo "" >> "$SUMMARY_FILE"
                          echo "Failed to process chunk ${CHUNK_INDEX} after ${MAX_RETRIES} attempts." >> "$SUMMARY_FILE"
                          echo "" >> "$SUMMARY_FILE"
                          echo "### Error Details" >> "$SUMMARY_FILE"
                          echo "" >> "$SUMMARY_FILE"
                          echo '```' >> "$SUMMARY_FILE"
                          cat "${RESULTS_DIR}/curl-error-${CHUNK_INDEX}.txt" >> "$SUMMARY_FILE"
                          echo '```' >> "$SUMMARY_FILE"
                          return 1
                      fi
                  fi
              done
          }
          
          # Process all chunks sequentially
          FAILED_CHUNKS=0
          SUCCESS_CHUNKS=0
          
          for (( i=0; i<$CHUNKS; i++ ))
          do
              # Add delay between chunks to avoid rate limiting
              if [ $i -gt 0 ]; then
                  echo "Waiting 5 seconds before processing next chunk..."
                  sleep 5
              fi
              
              # Process the chunk
              if process_chunk $i; then
                  SUCCESS_CHUNKS=$((SUCCESS_CHUNKS + 1))
              else
                  FAILED_CHUNKS=$((FAILED_CHUNKS + 1))
              fi
          done
          
          echo "Chunk processing complete: $SUCCESS_CHUNKS successful, $FAILED_CHUNKS failed"

      - name: Aggregate chunk results into final report
        env:
          GPT_FUNCTION_ENDPOINT: ${{ secrets.GPT_FUNCTION_ENDPOINT }}
          AZURE_BLOB_SAS_URL: ${{ secrets.AZURE_BLOB_SAS_URL }}
        run: |
          echo "Aggregating chunk results into final report"
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          
          # Initialize combined summary file
          COMBINED_SUMMARY="${RESULTS_DIR}/combined-summary-${TIMESTAMP}.md"
          
          # First check if we have enough successful chunks to proceed
          TOTAL_SUMMARY_FILES=$(find "${RESULTS_DIR}" -name "gpt-summary-*.md" | wc -l)
          if [ $TOTAL_SUMMARY_FILES -lt $(($CHUNKS / 2)) ]; then
              echo "ERROR: Too many failed chunks, cannot generate meaningful combined report"
              echo "# GPT-4 Security Analysis - Incomplete" > "$COMBINED_SUMMARY"
              echo "" >> "$COMBINED_SUMMARY"
              echo "Not enough chunks were successfully processed to generate a meaningful report." >> "$COMBINED_SUMMARY"
              echo "" >> "$COMBINED_SUMMARY"
              echo "Successfully processed: ${TOTAL_SUMMARY_FILES}/${CHUNKS} chunks" >> "$COMBINED_SUMMARY"
              exit 1
          fi
          
          # Approach 1: If limited chunks (<=3), combine them directly
          if [ $CHUNKS -le 3 ]; then
              echo "Combining a small number of chunks directly"
              
              echo "# Combined Security Analysis Report" > "$COMBINED_SUMMARY"
              echo "" >> "$COMBINED_SUMMARY"
              echo "This report combines analysis from ${TOTAL_SUMMARY_FILES} chunks." >> "$COMBINED_SUMMARY"
              echo "" >> "$COMBINED_SUMMARY"
              
              # Append each chunk's analysis to the combined summary
              for (( i=0; i<$CHUNKS; i++ ))
              do
                  CHUNK_SUMMARY="${RESULTS_DIR}/gpt-summary-${i}.md"
                  if [ -f "$CHUNK_SUMMARY" ]; then
                      echo "## Chunk $((i + 1)) Analysis" >> "$COMBINED_SUMMARY"
                      echo "" >> "$COMBINED_SUMMARY"
                      cat "$CHUNK_SUMMARY" >> "$COMBINED_SUMMARY"
                      echo "" >> "$COMBINED_SUMMARY"
                      echo "---" >> "$COMBINED_SUMMARY"
                      echo "" >> "$COMBINED_SUMMARY"
                  else
                      echo "## Chunk $((i + 1)) Analysis" >> "$COMBINED_SUMMARY"
                      echo "" >> "$COMBINED_SUMMARY"
                      echo "Analysis for this chunk is not available." >> "$COMBINED_SUMMARY"
                      echo "" >> "$COMBINED_SUMMARY"
                      echo "---" >> "$COMBINED_SUMMARY"
                      echo "" >> "$COMBINED_SUMMARY"
                  fi
              done
          # Approach 2: For more chunks, use GPT to synthesize the results
          else
              echo "Using GPT to synthesize results from ${CHUNKS} chunks"
              
              # Prepare summaries for the final prompt, properly escaped
              CHUNK_SUMMARIES=""
              for (( i=0; i<$CHUNKS; i++ ))
              do
                  CHUNK_SUMMARY="${RESULTS_DIR}/gpt-summary-${i}.md"
                  if [ -f "$CHUNK_SUMMARY" ]; then
                      # Create a clean summary with proper escaping to avoid sed issues
                      TEMP_SUMMARY="${RESULTS_DIR}/temp-summary-${i}.txt"
                      cat "$CHUNK_SUMMARY" | sed 's/\\/\\\\/g; s/\//\\\//g; s/&/\\&/g' > "$TEMP_SUMMARY"
                      CHUNK_SUMMARIES="${CHUNK_SUMMARIES}## Chunk $((i + 1)) Summary\n\n$(cat ${TEMP_SUMMARY})\n\n---\n\n"
                  fi
              done
              
              # Prepare the final GPT prompt for synthesizing results
              FINAL_PROMPT_FILE="${RESULTS_DIR}/final-prompt-${TIMESTAMP}.txt"
              # Use perl instead of sed for more reliable replacement of multi-line content
              perl -e '
                  local $/;
                  open(my $template, "<", "final-prompt.txt");
                  my $content = <$template>;
                  close($template);
                  my $summaries = $ENV{"CHUNK_SUMMARIES"};
                  $content =~ s/\{CHUNK_SUMMARIES\}/$summaries/g;
                  open(my $out, ">", "'"${FINAL_PROMPT_FILE}"'");
                  print $out $content;
                  close($out);
              '
              
              # Create payload for final GPT call
              jq -n \
                --rawfile prompt "$FINAL_PROMPT_FILE" \
                '{"prompt": $prompt}' > "${RESULTS_DIR}/final-gpt-payload-${TIMESTAMP}.json"
              
              echo "Calling GPT to synthesize the final report"
              MAX_RETRIES=3
              TIMEOUT=240  # Longer timeout for synthesis
              
              for try in $(seq 1 $MAX_RETRIES); do
                  if [ $try -gt 1 ]; then
                      SLEEP_TIME=$(( (try - 1) * 30 ))
                      echo "Retrying final synthesis after ${SLEEP_TIME}s delay (Attempt ${try}/${MAX_RETRIES})"
                      sleep $SLEEP_TIME
                  fi
                  
                  # Debug log the final synthesis payload size
                  echo "Submitting final synthesis of $(wc -c < "${RESULTS_DIR}/final-gpt-payload-${TIMESTAMP}.json") bytes to GPT API"
                  
                  # Save a copy of the final prompt for debugging
                  cp "${FINAL_PROMPT_FILE}" "${RESULTS_DIR}/final-prompt-debug-${TIMESTAMP}.txt"
                  
                  # Call the GPT-4 API for final synthesis with verbose logging
                  HTTP_CODE=$(curl -v -s -o "${RESULTS_DIR}/final-response-${TIMESTAMP}.txt" -w "%{http_code}" \
                      -X POST "${GPT_FUNCTION_ENDPOINT}" \
                      -H "Content-Type: application/json" \
                      -H "Accept: application/json" \
                      --data-binary @"${RESULTS_DIR}/final-gpt-payload-${TIMESTAMP}.json" \
                      --max-time $TIMEOUT 2> "${RESULTS_DIR}/final-curl-error-${TIMESTAMP}.txt")
                  
                  if [ "$HTTP_CODE" -eq 200 ]; then
                      echo "Final synthesis completed successfully"
                      
                      # Save raw response for debugging
                      cp "${RESULTS_DIR}/final-response-${TIMESTAMP}.txt" "${RESULTS_DIR}/final-raw-response-${TIMESTAMP}.json"
                      
                      # Process response with enhanced parsing logic
                      if grep -q "^#" "${RESULTS_DIR}/final-response-${TIMESTAMP}.txt"; then
                          echo "Found markdown response"
                          # Direct markdown response
                          cat "${RESULTS_DIR}/final-response-${TIMESTAMP}.txt" > "$COMBINED_SUMMARY"
                      elif grep -q "^{" "${RESULTS_DIR}/final-response-${TIMESTAMP}.txt"; then
                          echo "Found JSON response"
                          # Log JSON structure for debugging
                          jq 'keys' "${RESULTS_DIR}/final-response-${TIMESTAMP}.txt" > "${RESULTS_DIR}/final-keys-${TIMESTAMP}.txt" 2>/dev/null
                          
                          # Extract JSON content with more field patterns
                          if jq -r '.content // .result // .markdown // .text // .analysis // .output // .response // .message.content // .choices[0].message.content // .' "${RESULTS_DIR}/final-response-${TIMESTAMP}.txt" > "$COMBINED_SUMMARY" 2>/dev/null; then
                              echo "Extracted content from JSON response for final synthesis"
                          else
                              # Fall back to raw response
                              echo "JSON extraction failed - using raw output"
                              echo "# GPT-4 Combined Security Analysis" > "$COMBINED_SUMMARY"
                              cat "${RESULTS_DIR}/final-response-${TIMESTAMP}.txt" >> "$COMBINED_SUMMARY"
                          fi
                      else
                          echo "Found plain text response"
                          # Raw text response
                          echo "# GPT-4 Combined Security Analysis" > "$COMBINED_SUMMARY"
                          cat "${RESULTS_DIR}/final-response-${TIMESTAMP}.txt" >> "$COMBINED_SUMMARY"
                      fi
                      
                      break
                  else
                      echo "API request failed for final synthesis with HTTP code ${HTTP_CODE} (attempt ${try})"
                      echo "Error details:"
                      cat "${RESULTS_DIR}/final-curl-error-${TIMESTAMP}.txt"
                      
                      # Last retry failed, create manual combination
                      if [ $try -eq $MAX_RETRIES ]; then
                          echo "# Combined Security Analysis Report (Manual Aggregation)" > "$COMBINED_SUMMARY"
                          echo "" >> "$COMBINED_SUMMARY"
                          echo "GPT synthesis failed. This report combines individual chunk analyses without synthesis." >> "$COMBINED_SUMMARY"
                          echo "" >> "$COMBINED_SUMMARY"
                          
                          # Append each chunk's analysis to the combined summary
                          for (( i=0; i<$CHUNKS; i++ ))
                          do
                              CHUNK_SUMMARY="${RESULTS_DIR}/gpt-summary-${i}.md"
                              if [ -f "$CHUNK_SUMMARY" ]; then
                                  echo "## Chunk $((i + 1)) Analysis" >> "$COMBINED_SUMMARY"
                                  echo "" >> "$COMBINED_SUMMARY"
                                  cat "$CHUNK_SUMMARY" >> "$COMBINED_SUMMARY"
                                  echo "" >> "$COMBINED_SUMMARY"
                                  echo "---" >> "$COMBINED_SUMMARY"
                                  echo "" >> "$COMBINED_SUMMARY"
                              fi
                          done
                      fi
                  fi
              done
          fi
          
          # Create a metadata file with diagnostic information
          METADATA_FILE="${RESULTS_DIR}/pipeline-metadata-${TIMESTAMP}.json"
          jq -n \
            --arg timestamp "$(date)" \
            --arg chunkSize "$CHUNK_SIZE" \
            --arg totalIssues "$TOTAL_ISSUES" \
            --arg chunks "$CHUNKS" \
            --arg successChunks "$SUCCESS_CHUNKS" \
            --arg failedChunks "$FAILED_CHUNKS" \
            '{
              "timestamp": $timestamp,
              "totalIssues": $totalIssues,
              "chunkSize": $chunkSize,
              "totalChunks": $chunks,
              "successfulChunks": $successChunks,
              "failedChunks": $failedChunks
            }' > "$METADATA_FILE"
            
          # Also save a copy in the root for easy access
          cp "$COMBINED_SUMMARY" "gpt-summary.md"
          echo "Final combined report saved to gpt-summary.md and $COMBINED_SUMMARY"
          echo "Execution metadata saved to $METADATA_FILE"

      - name: Upload Final Report to Azure Blob Storage
        env:
          AZURE_BLOB_SAS_URL: ${{ secrets.AZURE_BLOB_SAS_URL }}
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          FILENAME="gpt-final-summary-${TIMESTAMP}.md"
          SAS_URL="${AZURE_BLOB_SAS_URL}"
          BASE_URL=$(echo "$SAS_URL" | cut -d'?' -f1)
          SAS_TOKEN=$(echo "$SAS_URL" | cut -d'?' -f2-)
          FULL_URL="${BASE_URL}/${FILENAME}?${SAS_TOKEN}"
          curl -s -S -f -X PUT "$FULL_URL" -H "x-ms-blob-type: BlockBlob" --data-binary @gpt-summary.md
          
          echo "Final analysis summary has been successfully uploaded to Azure Blob Storage as $FILENAME"
